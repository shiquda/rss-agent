#!/usr/bin/env python3
"""
RSS Agent CLI - ç»Ÿä¸€çš„ RSS è®¢é˜…ç®¡ç†å·¥å…·
Usage: rss <command> [options]
"""

import argparse
import json
import os
import sys
from datetime import datetime

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_DIR = os.path.expanduser("~/.openclaw/workspace")
FEEDS_FILE = os.path.join(CONFIG_DIR, "rss_feeds.json")

def load_feeds():
    """åŠ è½½è®¢é˜…åˆ—è¡¨"""
    if not os.path.exists(FEEDS_FILE):
        return []
    with open(FEEDS_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_feeds(feeds):
    """ä¿å­˜è®¢é˜…åˆ—è¡¨"""
    with open(FEEDS_FILE, 'w', encoding='utf-8') as f:
        json.dump(feeds, f, ensure_ascii=False, indent=2)

def cmd_list(args):
    """åˆ—å‡ºæ‰€æœ‰è®¢é˜…"""
    feeds = load_feeds()
    
    if args.category:
        feeds = [f for f in feeds if f.get('category') == args.category]
    
    if not feeds:
        print("ğŸ“­ æš‚æ— è®¢é˜…")
        return
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    categories = {}
    for feed in feeds:
        cat = feed.get('category') or 'æœªåˆ†ç±»'
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(feed)
    
    total = len(feeds)
    print(f"ğŸ“š å…± {total} ä¸ªè®¢é˜…\n")
    
    for cat, cat_feeds in sorted(categories.items()):
        print(f"\nã€{cat}ã€‘({len(cat_feeds)}ä¸ª)")
        print("-" * 40)
        for feed in cat_feeds:
            name = feed.get('name', 'Unknown')
            url = feed.get('xmlUrl', '')
            # æˆªæ–­é•¿ URL
            url_display = url[:50] + "..." if len(url) > 50 else url
            print(f"  â€¢ {name}")
            if args.verbose:
                print(f"    URL: {url_display}")

def cmd_add(args):
    """æ·»åŠ æ–°è®¢é˜…"""
    feeds = load_feeds()
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    for feed in feeds:
        if feed.get('xmlUrl') == args.url:
            print(f"âš ï¸ è¯¥è®¢é˜…å·²å­˜åœ¨: {feed.get('name')}")
            return
    
    new_feed = {
        "xmlUrl": args.url,
        "category": args.category or "æœªåˆ†ç±»"
    }
    
    if args.name:
        new_feed["name"] = args.name
    else:
        # å°è¯•ä» URL æå–åç§°
        from urllib.parse import urlparse
        parsed = urlparse(args.url)
        new_feed["name"] = parsed.netloc or "æœªå‘½å"
    
    if args.html_url:
        new_feed["htmlUrl"] = args.html_url
    
    feeds.append(new_feed)
    save_feeds(feeds)
    print(f"âœ… å·²æ·»åŠ è®¢é˜…: {new_feed['name']}")
    print(f"   åˆ†ç±»: {new_feed['category']}")

def cmd_remove(args):
    """åˆ é™¤è®¢é˜…"""
    feeds = load_feeds()
    
    # æ”¯æŒæŒ‰åç§°æˆ– URL åˆ é™¤
    removed = []
    remaining = []
    
    for feed in feeds:
        if feed.get('name') == args.identifier or feed.get('xmlUrl') == args.identifier:
            removed.append(feed)
        else:
            remaining.append(feed)
    
    if not removed:
        print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„è®¢é˜…: {args.identifier}")
        return
    
    save_feeds(remaining)
    for feed in removed:
        print(f"ğŸ—‘ï¸ å·²åˆ é™¤: {feed.get('name')}")

def cmd_check(args):
    """æ£€æŸ¥è®¢é˜…å¥åº·çŠ¶æ€"""
    import requests
    
    feeds = load_feeds()
    
    if not feeds:
        print("ğŸ“­ æš‚æ— è®¢é˜…")
        return
    
    print(f"ğŸ” æ­£åœ¨æ£€æŸ¥ {len(feeds)} ä¸ªè®¢é˜…...\n")
    
    ok_count = 0
    fail_count = 0
    
    for feed in feeds:
        name = feed.get('name', 'Unknown')
        url = feed.get('xmlUrl', '')
        
        try:
            resp = requests.get(url, timeout=10, 
                              headers={'User-Agent': 'OpenClaw-RSS-Agent/1.0'})
            if resp.status_code == 200:
                content_type = resp.headers.get('Content-Type', '').lower()
                is_xml = 'xml' in content_type or 'rss' in content_type or 'atom' in content_type
                if not is_xml:
                    is_xml = resp.text.strip().startswith('<?xml') or '<rss' in resp.text[:500]
                
                if is_xml:
                    print(f"âœ… {name}")
                    ok_count += 1
                else:
                    print(f"âš ï¸ {name} - è¿”å›å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„ RSS/Atom")
                    fail_count += 1
            else:
                print(f"âŒ {name} - HTTP {resp.status_code}")
                fail_count += 1
        except Exception as e:
            print(f"âŒ {name} - {str(e)[:50]}")
            fail_count += 1
    
    print(f"\nğŸ“Š æ£€æŸ¥ç»“æœ: {ok_count} æ­£å¸¸, {fail_count} å¼‚å¸¸")

def cmd_fetch(args):
    """è·å–è®¢é˜…å†…å®¹"""
    import requests
    import xml.etree.ElementTree as ET
    import html
    
    feeds = load_feeds()
    
    # æŸ¥æ‰¾åŒ¹é…çš„è®¢é˜…
    target_feed = None
    for feed in feeds:
        if feed.get('name') == args.identifier or feed.get('xmlUrl') == args.identifier:
            target_feed = feed
            break
    
    if not target_feed:
        print(f"âŒ æœªæ‰¾åˆ°è®¢é˜…: {args.identifier}")
        return
    
    url = target_feed.get('xmlUrl')
    name = target_feed.get('name')
    limit = args.limit
    full_content = args.full_content
    
    print(f"ğŸ“¡ æ­£åœ¨è·å–: {name}{' (å…¨æ–‡æ¨¡å¼)' if full_content else ''}\n")
    
    try:
        resp = requests.get(url, timeout=15, 
                          headers={'User-Agent': 'OpenClaw-RSS-Agent/1.0'})
        if resp.status_code != 200:
            print(f"âŒ HTTP {resp.status_code}")
            return
        
        root = ET.fromstring(resp.content)
        items = []
        
        # Namespaces
        content_ns = '{http://purl.org/rss/1.0/modules/content/}'
        atom_ns = '{http://www.w3.org/2005/Atom}'
        
        # RSS 2.0
        channel = root.find('channel')
        if channel is not None:
            for item in channel.findall('item')[:limit]:
                title = item.findtext('title', 'No Title')
                link = item.findtext('link', '')
                pub_date = item.findtext('pubDate', '')
                desc = item.findtext('description', '')
                
                content = None
                if full_content:
                    content_elem = item.find(f'{content_ns}encoded')
                    if content_elem is not None and content_elem.text:
                        content = html.unescape(content_elem.text)
                
                items.append({
                    "title": title, 
                    "link": link, 
                    "date": pub_date,
                    "summary": desc[:300] + "..." if len(desc) > 300 else desc,
                    "content": content
                })
        else:
            # Atom
            entries = root.findall(f'{atom_ns}entry')
            for entry in entries[:limit]:
                title = entry.findtext(f'{atom_ns}title', 'No Title')
                link_node = entry.find(f'{atom_ns}link')
                link = link_node.get('href') if link_node is not None else ''
                pub_date = entry.findtext(f'{atom_ns}updated', '')
                summary = entry.findtext(f'{atom_ns}summary', '')
                
                content = None
                if full_content:
                    content_elem = entry.find(f'{atom_ns}content')
                    if content_elem is not None and content_elem.text:
                        content = html.unescape(content_elem.text)
                
                items.append({
                    "title": title, 
                    "link": link, 
                    "date": pub_date,
                    "summary": summary[:300] + "..." if len(summary) > 300 else summary,
                    "content": content
                })
        
        print(f"ğŸ“° æœ€æ–° {len(items)} æ¡å†…å®¹:\n")
        for i, item in enumerate(items, 1):
            print(f"{'='*50}")
            print(f"{i}. {item['title']}")
            if item['date']:
                print(f"   æ—¶é—´: {item['date']}")
            if args.verbose and item['link']:
                print(f"   é“¾æ¥: {item['link']}")
            
            if full_content and item['content']:
                # Strip HTML tags for readability
                content = item['content']
                content = content.replace('<p>', '\n').replace('</p>', '')
                content = content.replace('<br>', '\n').replace('<br/>', '\n')
                # Simple HTML tag removal
                import re
                content = re.sub('<[^<]+?>', '', content)
                content = html.unescape(content)
                print(f"\nğŸ“„ å…¨æ–‡å†…å®¹:\n{content[:2000]}..." if len(content) > 2000 else f"\nğŸ“„ å…¨æ–‡å†…å®¹:\n{content}")
            elif not full_content:
                print(f"\nğŸ“ æ‘˜è¦: {item['summary']}")
            else:
                print(f"\nâš ï¸ è¯¥æºæœªæä¾›å…¨æ–‡å†…å®¹")
            print()
            
    except Exception as e:
        print(f"âŒ è·å–å¤±è´¥: {e}")

def cmd_export(args):
    """å¯¼å‡ºä¸º OPML"""
    from xml.etree.ElementTree import Element, SubElement, tostring
    from xml.dom import minidom
    
    feeds = load_feeds()
    
    if not feeds:
        print("ğŸ“­ æš‚æ— è®¢é˜…å¯å¯¼å‡º")
        return
    
    # åˆ›å»º OPML
    opml = Element('opml', version='2.0')
    
    head = SubElement(opml, 'head')
    title = SubElement(head, 'title')
    title.text = 'RSS Subscriptions'
    date_created = SubElement(head, 'dateCreated')
    date_created.text = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    
    body = SubElement(opml, 'body')
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    categories = {}
    for feed in feeds:
        cat = feed.get('category') or 'æœªåˆ†ç±»'
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(feed)
    
    for category, cat_feeds in sorted(categories.items()):
        if len(categories) > 1:
            cat_outline = SubElement(body, 'outline', text=category, title=category)
            parent = cat_outline
        else:
            parent = body
        
        for feed in cat_feeds:
            attrs = {
                'type': 'rss',
                'text': feed.get('name', 'Unknown'),
                'title': feed.get('name', 'Unknown'),
                'xmlUrl': feed.get('xmlUrl', '')
            }
            if feed.get('htmlUrl'):
                attrs['htmlUrl'] = feed['htmlUrl']
            SubElement(parent, 'outline', **attrs)
    
    # æ ¼å¼åŒ–è¾“å‡º
    xml_str = tostring(opml, encoding='utf-8')
    dom = minidom.parseString(xml_str)
    pretty_xml = dom.toprettyxml(indent='  ', encoding='utf-8')
    lines = [line for line in pretty_xml.decode('utf-8').split('\n') if line.strip()]
    pretty_xml = '\n'.join(lines)
    
    output_file = args.output or f'rss_export_{datetime.now().strftime("%Y%m%d")}.opml'
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(pretty_xml)
    
    print(f"âœ… å·²å¯¼å‡º: {output_file}")
    print(f"ğŸ“Š æ€»è®¡ {len(feeds)} ä¸ªè®¢é˜…ï¼Œ{len(categories)} ä¸ªåˆ†ç±»")

def cmd_import(args):
    """ä» OPML å¯¼å…¥"""
    import xml.etree.ElementTree as ET
    
    if not os.path.exists(args.file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
        return
    
    try:
        tree = ET.parse(args.file)
        root = tree.getroot()
        
        new_feeds = []
        
        def walk(node, category=None):
            for outline in node.findall('outline'):
                text = outline.get('text')
                xml_url = outline.get('xmlUrl')
                html_url = outline.get('htmlUrl')
                
                if xml_url:
                    new_feeds.append({
                        "name": text,
                        "xmlUrl": xml_url,
                        "htmlUrl": html_url,
                        "category": category or "æœªåˆ†ç±»"
                    })
                
                # é€’å½’å¤„ç†åµŒå¥—åˆ†ç±»
                walk(outline, category=text if not xml_url else category)
        
        walk(root.find('body'))
        
        if not new_feeds:
            print("âš ï¸ æœªåœ¨ OPML ä¸­æ‰¾åˆ°è®¢é˜…æº")
            return
        
        # åˆå¹¶ç°æœ‰è®¢é˜…
        existing_feeds = load_feeds()
        existing_urls = {f.get('xmlUrl') for f in existing_feeds}
        
        added = 0
        skipped = 0
        
        for feed in new_feeds:
            if feed['xmlUrl'] not in existing_urls:
                existing_feeds.append(feed)
                added += 1
            else:
                skipped += 1
        
        save_feeds(existing_feeds)
        print(f"âœ… å¯¼å…¥å®Œæˆ: æ–°å¢ {added} ä¸ª, è·³è¿‡ {skipped} ä¸ªé‡å¤")
        
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")

def cmd_digest(args):
    """è·å–ä»Šæ—¥/æ˜¨æ—¥æ›´æ–°çš„æ‘˜è¦"""
    import requests
    import xml.etree.ElementTree as ET
    from datetime import datetime, timedelta
    import re
    
    feeds = load_feeds()
    
    if not feeds:
        print("ğŸ“­ æš‚æ— è®¢é˜…")
        return
    
    # æŒ‰åˆ†ç±»ç­›é€‰
    if args.category:
        feeds = [f for f in feeds if f.get('category') == args.category]
        if not feeds:
            print(f"ğŸ“­ åˆ†ç±» '{args.category}' ä¸‹æš‚æ— è®¢é˜…")
            return
    
    # è®¡ç®—æ—¶é—´èŒƒå›´
    now = datetime.now()
    if args.days:
        since = now - timedelta(days=args.days)
    else:
        # é»˜è®¤ä»ä»Šå¤© 00:00 å¼€å§‹
        since = now.replace(hour=0, minute=0, second=0, microsecond=0)
    
    print(f"ğŸ“… è·å–æ›´æ–°: {since.strftime('%Y-%m-%d %H:%M')} â†’ {now.strftime('%Y-%m-%d %H:%M')}\n")
    
    all_updates = []
    
    # é™åˆ¶æ£€æŸ¥æ•°é‡
    if args.max_feeds > 0:
        feeds = feeds[:args.max_feeds]
    
    for feed in feeds:
        name = feed.get('name', 'Unknown')
        url = feed.get('xmlUrl', '')
        category = feed.get('category', 'æœªåˆ†ç±»')
        
        try:
            resp = requests.get(url, timeout=10, 
                              headers={'User-Agent': 'OpenClaw-RSS-Agent/1.0'})
            if resp.status_code != 200:
                continue
            
            root = ET.fromstring(resp.content)
            items = []
            
            content_ns = '{http://purl.org/rss/1.0/modules/content/}'
            atom_ns = '{http://www.w3.org/2005/Atom}'
            
            # RSS 2.0
            channel = root.find('channel')
            if channel is not None:
                for item in channel.findall('item'):
                    title = item.findtext('title', 'No Title')
                    link = item.findtext('link', '')
                    pub_date = item.findtext('pubDate', '')
                    
                    if pub_date:
                        try:
                            # å°è¯•è§£æ RSS æ—¥æœŸæ ¼å¼
                            from email.utils import parsedate_to_datetime
                            from datetime import timezone
                            item_date = parsedate_to_datetime(pub_date)
                            # è½¬æ¢ä¸ºæ— æ—¶åŒºæ—¶é—´è¿›è¡Œæ¯”è¾ƒ
                            if item_date.tzinfo:
                                item_date = item_date.replace(tzinfo=None)
                            if item_date >= since:
                                items.append({
                                    'title': title,
                                    'link': link,
                                    'date': item_date,
                                    'feed_name': name,
                                    'category': category
                                })
                        except:
                            pass
            else:
                # Atom
                entries = root.findall(f'{atom_ns}entry')
                for entry in entries:
                    title = entry.findtext(f'{atom_ns}title', 'No Title')
                    link_node = entry.find(f'{atom_ns}link')
                    link = link_node.get('href') if link_node is not None else ''
                    pub_date = entry.findtext(f'{atom_ns}updated', '')
                    
                    if pub_date:
                        try:
                            item_date = datetime.fromisoformat(pub_date.replace('Z', '+00:00').replace('+00:00', ''))
                            if item_date >= since:
                                items.append({
                                    'title': title,
                                    'link': link,
                                    'date': item_date,
                                    'feed_name': name,
                                    'category': category
                                })
                        except:
                            pass
            
            all_updates.extend(items)
            
        except Exception as e:
            pass  # é™é»˜è·³è¿‡å¤±è´¥çš„æº
    
    # æŒ‰æ—¶é—´æ’åº
    all_updates.sort(key=lambda x: x['date'], reverse=True)
    
    if not all_updates:
        print("ğŸ“­ è¯¥æ—¶é—´æ®µå†…æ— æ–°å†…å®¹")
        return
    
    # æŒ‰åˆ†ç±»åˆ†ç»„æ˜¾ç¤º
    by_category = {}
    for item in all_updates:
        cat = item['category']
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(item)
    
    print(f"ğŸ“Š å…± {len(all_updates)} æ¡æ–°å†…å®¹\n")
    print("="*60)
    
    for category in sorted(by_category.keys()):
        items = by_category[category]
        print(f"\nã€{category}ã€‘({len(items)}æ¡)")
        print("-"*40)
        
        for item in items[:args.limit]:
            time_str = item['date'].strftime('%m-%d %H:%M')
            print(f"  â€¢ [{time_str}] {item['title'][:50]}{'...' if len(item['title']) > 50 else ''}")
            print(f"    æ¥æº: {item['feed_name']}")
            if args.verbose and item['link']:
                print(f"    é“¾æ¥: {item['link']}")
        
        if len(items) > args.limit:
            print(f"    ... è¿˜æœ‰ {len(items) - args.limit} æ¡")
    
    print(f"\n{'='*60}")
    print(f"ğŸ• æ›´æ–°æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M')}")

def main():
    parser = argparse.ArgumentParser(
        prog='rss',
        description='RSS Agent CLI - ç®¡ç†ä½ çš„ RSS è®¢é˜…',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  rss list                      # åˆ—å‡ºæ‰€æœ‰è®¢é˜…
  rss list --category åšå®¢       # æŒ‰åˆ†ç±»ç­›é€‰
  rss add https://example.com/feed.xml --category ç§‘æŠ€
  rss remove "æŸä¸ªè®¢é˜…åç§°"
  rss check                     # æ£€æŸ¥æ‰€æœ‰è®¢é˜…çŠ¶æ€
  rss fetch "Feed Name" --limit 3      # è·å–æŸè®¢é˜…æœ€æ–°3æ¡
  rss digest                    # è·å–ä»Šæ—¥æ‰€æœ‰æ›´æ–°
  rss digest -d 2               # è·å–æœ€è¿‘2å¤©çš„æ›´æ–°
  rss export                    # å¯¼å‡ºä¸º OPML
  rss import follow.opml        # ä» OPML å¯¼å…¥
        '''
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # list å‘½ä»¤
    list_parser = subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰è®¢é˜…')
    list_parser.add_argument('-c', '--category', help='æŒ‰åˆ†ç±»ç­›é€‰')
    list_parser.add_argument('-v', '--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    
    # add å‘½ä»¤
    add_parser = subparsers.add_parser('add', help='æ·»åŠ è®¢é˜…')
    add_parser.add_argument('url', help='RSS feed URL')
    add_parser.add_argument('-n', '--name', help='è‡ªå®šä¹‰åç§°')
    add_parser.add_argument('-c', '--category', help='åˆ†ç±»')
    add_parser.add_argument('--html-url', help='ç½‘ç«™ä¸»é¡µ URL')
    
    # remove å‘½ä»¤
    remove_parser = subparsers.add_parser('remove', help='åˆ é™¤è®¢é˜…')
    remove_parser.add_argument('identifier', help='è®¢é˜…åç§°æˆ– URL')
    
    # check å‘½ä»¤
    check_parser = subparsers.add_parser('check', help='æ£€æŸ¥è®¢é˜…å¥åº·çŠ¶æ€')
    
    # fetch å‘½ä»¤
    fetch_parser = subparsers.add_parser('fetch', help='è·å–è®¢é˜…å†…å®¹')
    fetch_parser.add_argument('identifier', help='è®¢é˜…åç§°æˆ– URL')
    fetch_parser.add_argument('-n', '--limit', type=int, default=5, help='è·å–æ•°é‡ (é»˜è®¤5)')
    fetch_parser.add_argument('-v', '--verbose', action='store_true', help='æ˜¾ç¤ºé“¾æ¥')
    fetch_parser.add_argument('--full-content', action='store_true', help='å°è¯•è·å–å…¨æ–‡å†…å®¹ (ä»…éƒ¨åˆ†æºæ”¯æŒ)')
    
    # export å‘½ä»¤
    export_parser = subparsers.add_parser('export', help='å¯¼å‡º OPML')
    export_parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶å')
    
    # import å‘½ä»¤
    import_parser = subparsers.add_parser('import', help='å¯¼å…¥ OPML')
    import_parser.add_argument('file', help='OPML æ–‡ä»¶è·¯å¾„')
    
    # digest å‘½ä»¤ - è·å–ä»Šæ—¥æ›´æ–°æ‘˜è¦
    digest_parser = subparsers.add_parser('digest', help='è·å–ä»Šæ—¥/æ˜¨æ—¥æ›´æ–°çš„æ‘˜è¦')
    digest_parser.add_argument('-d', '--days', type=int, help='è·å–æœ€è¿‘ N å¤©çš„å†…å®¹')
    digest_parser.add_argument('-n', '--limit', type=int, default=3, help='æ¯åˆ†ç±»æ˜¾ç¤ºæ•°é‡ (é»˜è®¤3)')
    digest_parser.add_argument('-c', '--category', help='ä»…è·å–æŒ‡å®šåˆ†ç±»')
    digest_parser.add_argument('-v', '--verbose', action='store_true', help='æ˜¾ç¤ºé“¾æ¥')
    digest_parser.add_argument('--max-feeds', type=int, default=0, help='æœ€å¤šæ£€æŸ¥Nä¸ªè®¢é˜…æº(0=å…¨éƒ¨)')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # æ‰§è¡Œå¯¹åº”å‘½ä»¤
    commands = {
        'list': cmd_list,
        'add': cmd_add,
        'remove': cmd_remove,
        'check': cmd_check,
        'fetch': cmd_fetch,
        'export': cmd_export,
        'import': cmd_import,
        'digest': cmd_digest,
    }
    
    commands[args.command](args)

if __name__ == '__main__':
    main()
