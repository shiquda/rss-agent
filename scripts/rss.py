#!/usr/bin/env python3
"""
RSS Agent CLI - ç»Ÿä¸€çš„ RSS è®¢é˜…ç®¡ç†å·¥å…·
Usage: rss <command> [options]
"""

import argparse
import json
import os
import sys
from datetime import datetime

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_DIR = os.path.expanduser("~/.openclaw/workspace")
FEEDS_FILE = os.path.join(CONFIG_DIR, "rss_feeds.json")

def load_feeds():
    """åŠ è½½è®¢é˜…åˆ—è¡¨"""
    if not os.path.exists(FEEDS_FILE):
        return []
    with open(FEEDS_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_feeds(feeds):
    """ä¿å­˜è®¢é˜…åˆ—è¡¨"""
    with open(FEEDS_FILE, 'w', encoding='utf-8') as f:
        json.dump(feeds, f, ensure_ascii=False, indent=2)

def cmd_list(args):
    """åˆ—å‡ºæ‰€æœ‰è®¢é˜…"""
    feeds = load_feeds()
    
    if args.category:
        feeds = [f for f in feeds if f.get('category') == args.category]
    
    if not feeds:
        print("ğŸ“­ æš‚æ— è®¢é˜…")
        return
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    categories = {}
    for feed in feeds:
        cat = feed.get('category') or 'æœªåˆ†ç±»'
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(feed)
    
    total = len(feeds)
    print(f"ğŸ“š å…± {total} ä¸ªè®¢é˜…\n")
    
    for cat, cat_feeds in sorted(categories.items()):
        print(f"\nã€{cat}ã€‘({len(cat_feeds)}ä¸ª)")
        print("-" * 40)
        for feed in cat_feeds:
            name = feed.get('name', 'Unknown')
            url = feed.get('xmlUrl', '')
            # æˆªæ–­é•¿ URL
            url_display = url[:50] + "..." if len(url) > 50 else url
            print(f"  â€¢ {name}")
            if args.verbose:
                print(f"    URL: {url_display}")

def cmd_add(args):
    """æ·»åŠ æ–°è®¢é˜…"""
    feeds = load_feeds()
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    for feed in feeds:
        if feed.get('xmlUrl') == args.url:
            print(f"âš ï¸ è¯¥è®¢é˜…å·²å­˜åœ¨: {feed.get('name')}")
            return
    
    new_feed = {
        "xmlUrl": args.url,
        "category": args.category or "æœªåˆ†ç±»"
    }
    
    if args.name:
        new_feed["name"] = args.name
    else:
        # å°è¯•ä» URL æå–åç§°
        from urllib.parse import urlparse
        parsed = urlparse(args.url)
        new_feed["name"] = parsed.netloc or "æœªå‘½å"
    
    if args.html_url:
        new_feed["htmlUrl"] = args.html_url
    
    feeds.append(new_feed)
    save_feeds(feeds)
    print(f"âœ… å·²æ·»åŠ è®¢é˜…: {new_feed['name']}")
    print(f"   åˆ†ç±»: {new_feed['category']}")

def cmd_remove(args):
    """åˆ é™¤è®¢é˜…"""
    feeds = load_feeds()
    
    # æ”¯æŒæŒ‰åç§°æˆ– URL åˆ é™¤
    removed = []
    remaining = []
    
    for feed in feeds:
        if feed.get('name') == args.identifier or feed.get('xmlUrl') == args.identifier:
            removed.append(feed)
        else:
            remaining.append(feed)
    
    if not removed:
        print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„è®¢é˜…: {args.identifier}")
        return
    
    save_feeds(remaining)
    for feed in removed:
        print(f"ğŸ—‘ï¸ å·²åˆ é™¤: {feed.get('name')}")

def cmd_check(args):
    """æ£€æŸ¥è®¢é˜…å¥åº·çŠ¶æ€"""
    import requests
    
    feeds = load_feeds()
    
    if not feeds:
        print("ğŸ“­ æš‚æ— è®¢é˜…")
        return
    
    print(f"ğŸ” æ­£åœ¨æ£€æŸ¥ {len(feeds)} ä¸ªè®¢é˜…...\n")
    
    ok_count = 0
    fail_count = 0
    
    for feed in feeds:
        name = feed.get('name', 'Unknown')
        url = feed.get('xmlUrl', '')
        
        try:
            resp = requests.get(url, timeout=10, 
                              headers={'User-Agent': 'OpenClaw-RSS-Agent/1.0'})
            if resp.status_code == 200:
                content_type = resp.headers.get('Content-Type', '').lower()
                is_xml = 'xml' in content_type or 'rss' in content_type or 'atom' in content_type
                if not is_xml:
                    is_xml = resp.text.strip().startswith('<?xml') or '<rss' in resp.text[:500]
                
                if is_xml:
                    print(f"âœ… {name}")
                    ok_count += 1
                else:
                    print(f"âš ï¸ {name} - è¿”å›å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„ RSS/Atom")
                    fail_count += 1
            else:
                print(f"âŒ {name} - HTTP {resp.status_code}")
                fail_count += 1
        except Exception as e:
            print(f"âŒ {name} - {str(e)[:50]}")
            fail_count += 1
    
    print(f"\nğŸ“Š æ£€æŸ¥ç»“æœ: {ok_count} æ­£å¸¸, {fail_count} å¼‚å¸¸")

def cmd_fetch(args):
    """è·å–è®¢é˜…å†…å®¹"""
    import requests
    import xml.etree.ElementTree as ET
    
    feeds = load_feeds()
    
    # æŸ¥æ‰¾åŒ¹é…çš„è®¢é˜…
    target_feed = None
    for feed in feeds:
        if feed.get('name') == args.identifier or feed.get('xmlUrl') == args.identifier:
            target_feed = feed
            break
    
    if not target_feed:
        print(f"âŒ æœªæ‰¾åˆ°è®¢é˜…: {args.identifier}")
        return
    
    url = target_feed.get('xmlUrl')
    name = target_feed.get('name')
    limit = args.limit
    
    print(f"ğŸ“¡ æ­£åœ¨è·å–: {name}\n")
    
    try:
        resp = requests.get(url, timeout=15, 
                          headers={'User-Agent': 'OpenClaw-RSS-Agent/1.0'})
        if resp.status_code != 200:
            print(f"âŒ HTTP {resp.status_code}")
            return
        
        root = ET.fromstring(resp.content)
        items = []
        
        # RSS 2.0
        channel = root.find('channel')
        if channel is not None:
            for item in channel.findall('item')[:limit]:
                title = item.findtext('title', 'No Title')
                link = item.findtext('link', '')
                pub_date = item.findtext('pubDate', '')
                items.append({"title": title, "link": link, "date": pub_date})
        else:
            # Atom
            entries = root.findall('{http://www.w3.org/2005/Atom}entry')
            for entry in entries[:limit]:
                title = entry.findtext('{http://www.w3.org/2005/Atom}title', 'No Title')
                link_node = entry.find('{http://www.w3.org/2005/Atom}link')
                link = link_node.get('href') if link_node is not None else ''
                pub_date = entry.findtext('{http://www.w3.org/2005/Atom}updated', '')
                items.append({"title": title, "link": link, "date": pub_date})
        
        print(f"ğŸ“° æœ€æ–° {len(items)} æ¡å†…å®¹:\n")
        for i, item in enumerate(items, 1):
            print(f"{i}. {item['title']}")
            if item['date']:
                print(f"   æ—¶é—´: {item['date']}")
            if args.verbose and item['link']:
                print(f"   é“¾æ¥: {item['link']}")
            print()
            
    except Exception as e:
        print(f"âŒ è·å–å¤±è´¥: {e}")

def cmd_export(args):
    """å¯¼å‡ºä¸º OPML"""
    from xml.etree.ElementTree import Element, SubElement, tostring
    from xml.dom import minidom
    
    feeds = load_feeds()
    
    if not feeds:
        print("ğŸ“­ æš‚æ— è®¢é˜…å¯å¯¼å‡º")
        return
    
    # åˆ›å»º OPML
    opml = Element('opml', version='2.0')
    
    head = SubElement(opml, 'head')
    title = SubElement(head, 'title')
    title.text = 'RSS Subscriptions'
    date_created = SubElement(head, 'dateCreated')
    date_created.text = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    
    body = SubElement(opml, 'body')
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    categories = {}
    for feed in feeds:
        cat = feed.get('category') or 'æœªåˆ†ç±»'
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(feed)
    
    for category, cat_feeds in sorted(categories.items()):
        if len(categories) > 1:
            cat_outline = SubElement(body, 'outline', text=category, title=category)
            parent = cat_outline
        else:
            parent = body
        
        for feed in cat_feeds:
            attrs = {
                'type': 'rss',
                'text': feed.get('name', 'Unknown'),
                'title': feed.get('name', 'Unknown'),
                'xmlUrl': feed.get('xmlUrl', '')
            }
            if feed.get('htmlUrl'):
                attrs['htmlUrl'] = feed['htmlUrl']
            SubElement(parent, 'outline', **attrs)
    
    # æ ¼å¼åŒ–è¾“å‡º
    xml_str = tostring(opml, encoding='utf-8')
    dom = minidom.parseString(xml_str)
    pretty_xml = dom.toprettyxml(indent='  ', encoding='utf-8')
    lines = [line for line in pretty_xml.decode('utf-8').split('\n') if line.strip()]
    pretty_xml = '\n'.join(lines)
    
    output_file = args.output or f'rss_export_{datetime.now().strftime("%Y%m%d")}.opml'
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(pretty_xml)
    
    print(f"âœ… å·²å¯¼å‡º: {output_file}")
    print(f"ğŸ“Š æ€»è®¡ {len(feeds)} ä¸ªè®¢é˜…ï¼Œ{len(categories)} ä¸ªåˆ†ç±»")

def cmd_import(args):
    """ä» OPML å¯¼å…¥"""
    import xml.etree.ElementTree as ET
    
    if not os.path.exists(args.file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
        return
    
    try:
        tree = ET.parse(args.file)
        root = tree.getroot()
        
        new_feeds = []
        
        def walk(node, category=None):
            for outline in node.findall('outline'):
                text = outline.get('text')
                xml_url = outline.get('xmlUrl')
                html_url = outline.get('htmlUrl')
                
                if xml_url:
                    new_feeds.append({
                        "name": text,
                        "xmlUrl": xml_url,
                        "htmlUrl": html_url,
                        "category": category or "æœªåˆ†ç±»"
                    })
                
                # é€’å½’å¤„ç†åµŒå¥—åˆ†ç±»
                walk(outline, category=text if not xml_url else category)
        
        walk(root.find('body'))
        
        if not new_feeds:
            print("âš ï¸ æœªåœ¨ OPML ä¸­æ‰¾åˆ°è®¢é˜…æº")
            return
        
        # åˆå¹¶ç°æœ‰è®¢é˜…
        existing_feeds = load_feeds()
        existing_urls = {f.get('xmlUrl') for f in existing_feeds}
        
        added = 0
        skipped = 0
        
        for feed in new_feeds:
            if feed['xmlUrl'] not in existing_urls:
                existing_feeds.append(feed)
                added += 1
            else:
                skipped += 1
        
        save_feeds(existing_feeds)
        print(f"âœ… å¯¼å…¥å®Œæˆ: æ–°å¢ {added} ä¸ª, è·³è¿‡ {skipped} ä¸ªé‡å¤")
        
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")

def main():
    parser = argparse.ArgumentParser(
        prog='rss',
        description='RSS Agent CLI - ç®¡ç†ä½ çš„ RSS è®¢é˜…',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  rss list                      # åˆ—å‡ºæ‰€æœ‰è®¢é˜…
  rss list --category åšå®¢       # æŒ‰åˆ†ç±»ç­›é€‰
  rss add https://example.com/feed.xml --category ç§‘æŠ€
  rss remove "æŸä¸ªè®¢é˜…åç§°"
  rss check                     # æ£€æŸ¥æ‰€æœ‰è®¢é˜…çŠ¶æ€
  rss fetch "DIYgod" --limit 3  # è·å–æŸè®¢é˜…æœ€æ–°3æ¡
  rss export                    # å¯¼å‡ºä¸º OPML
  rss import follow.opml        # ä» OPML å¯¼å…¥
        '''
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # list å‘½ä»¤
    list_parser = subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰è®¢é˜…')
    list_parser.add_argument('-c', '--category', help='æŒ‰åˆ†ç±»ç­›é€‰')
    list_parser.add_argument('-v', '--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    
    # add å‘½ä»¤
    add_parser = subparsers.add_parser('add', help='æ·»åŠ è®¢é˜…')
    add_parser.add_argument('url', help='RSS feed URL')
    add_parser.add_argument('-n', '--name', help='è‡ªå®šä¹‰åç§°')
    add_parser.add_argument('-c', '--category', help='åˆ†ç±»')
    add_parser.add_argument('--html-url', help='ç½‘ç«™ä¸»é¡µ URL')
    
    # remove å‘½ä»¤
    remove_parser = subparsers.add_parser('remove', help='åˆ é™¤è®¢é˜…')
    remove_parser.add_argument('identifier', help='è®¢é˜…åç§°æˆ– URL')
    
    # check å‘½ä»¤
    check_parser = subparsers.add_parser('check', help='æ£€æŸ¥è®¢é˜…å¥åº·çŠ¶æ€')
    
    # fetch å‘½ä»¤
    fetch_parser = subparsers.add_parser('fetch', help='è·å–è®¢é˜…å†…å®¹')
    fetch_parser.add_argument('identifier', help='è®¢é˜…åç§°æˆ– URL')
    fetch_parser.add_argument('-n', '--limit', type=int, default=5, help='è·å–æ•°é‡ (é»˜è®¤5)')
    fetch_parser.add_argument('-v', '--verbose', action='store_true', help='æ˜¾ç¤ºé“¾æ¥')
    
    # export å‘½ä»¤
    export_parser = subparsers.add_parser('export', help='å¯¼å‡º OPML')
    export_parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶å')
    
    # import å‘½ä»¤
    import_parser = subparsers.add_parser('import', help='å¯¼å…¥ OPML')
    import_parser.add_argument('file', help='OPML æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # æ‰§è¡Œå¯¹åº”å‘½ä»¤
    commands = {
        'list': cmd_list,
        'add': cmd_add,
        'remove': cmd_remove,
        'check': cmd_check,
        'fetch': cmd_fetch,
        'export': cmd_export,
        'import': cmd_import,
    }
    
    commands[args.command](args)

if __name__ == '__main__':
    main()
